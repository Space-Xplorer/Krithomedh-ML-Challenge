"""
Ensemble and Prediction Module (v3 - Segmented MAE Optimization)
- OOF (Out-of-Fold) prediction approach eliminates data leakage
- SEGMENTED ensemble weights: separate optimization for Numeric vs Alphanumeric IDs
- Segmented outlier capping: different caps for different ID types
- Proper cross-validation: each model only predicts on its held-out fold
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.amp import autocast
from transformers import AutoTokenizer
from sklearn.metrics import mean_absolute_error
from scipy.optimize import minimize
import joblib
import os
import sys

# Import custom modules
sys.path.append(os.path.dirname(__file__))
from train_deberta import DeBERTaRegressor, ScoreDataset


def load_deberta_model(model_path, device='cuda'):
    """Load trained DeBERTa model."""
    checkpoint = torch.load(model_path, map_location=device, weights_only=False)
    n_features = checkpoint['n_features']
    
    model = DeBERTaRegressor(n_features=n_features)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()
    
    return model, checkpoint.get('scaler'), checkpoint.get('feature_cols')


def predict_deberta(model, texts, features, tokenizer, device, batch_size=16):
    """Generate predictions using DeBERTa model with AMP."""
    dataset = ScoreDataset(texts, np.zeros(len(texts)), tokenizer, features=features)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
    
    use_amp = device.type == 'cuda'
    predictions = []
    model.eval()
    
    with torch.no_grad():
        for batch in loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            feats = batch.get('features')
            if feats is not None:
                feats = feats.to(device)
            
            with autocast('cuda', enabled=use_amp):
                preds = model(input_ids, attention_mask, feats)
            predictions.extend(preds.float().cpu().numpy())
    
    return np.array(predictions)


def optimize_ensemble_weights(predictions_list, y_true, model_names=None):
    """
    Optimize ensemble weights using MAE (competition metric).
    
    Args:
        predictions_list: List of prediction arrays from different models
        y_true: True target values (log space)
        model_names: Names for display
    
    Returns:
        Optimal weights array
    """
    print("\n[Optimizing Ensemble Weights (MAE)]")
    
    predictions_array = np.array(predictions_list)
    
    def objective(weights):
        """Minimize MAE in ORIGINAL space â€” the actual competition metric."""
        ensemble_pred_log = np.average(predictions_array, axis=0, weights=weights)
        ensemble_pred_orig = np.expm1(np.clip(ensemble_pred_log, 0, 20))
        y_true_orig = np.expm1(y_true)
        return mean_absolute_error(y_true_orig, ensemble_pred_orig)
    
    # Constraints: weights sum to 1, all weights >= 0
    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}
    bounds = [(0, 1) for _ in range(len(predictions_list))]
    
    # Initial guess: equal weights
    x0 = np.ones(len(predictions_list)) / len(predictions_list)
    
    # Optimize with multiple restarts for robustness
    best_result = None
    best_mae = float('inf')
    
    for seed in range(5):
        np.random.seed(seed)
        if seed == 0:
            init = x0
        else:
            init = np.random.dirichlet(np.ones(len(predictions_list)))
        
        result = minimize(
            objective,
            init,
            method='SLSQP',
            bounds=bounds,
            constraints=constraints
        )
        
        if result.fun < best_mae:
            best_mae = result.fun
            best_result = result
    
    optimal_weights = best_result.x
    final_mae = best_result.fun
    
    if model_names is None:
        model_names = [f'Model {i}' for i in range(len(optimal_weights))]
    
    print("Optimized Weights:")
    for name, weight in zip(model_names, optimal_weights):
        print(f"  {name}: {weight:.4f}")
    print(f"\nEnsemble MAE (original space): {final_mae:.2f}")
    
    return optimal_weights


def optimize_segmented_weights(predictions_list, y_true, id_series, model_names=None):
    """
    Optimize ensemble weights SEPARATELY for Numeric vs Alphanumeric IDs.
    
    This is the breakthrough technique for sub-1085 MAE.
    
    Args:
        predictions_list: List of prediction arrays (log space)
        y_true: True target values (log space)
        id_series: Series/array of IDs to determine segmentation
        model_names: Names for display
        
    Returns:
        Dictionary with 'numeric' and 'alphanumeric' weight arrays
    """
    print("\n" + "="*60)
    print("SEGMENTED ENSEMBLE WEIGHT OPTIMIZATION")
    print("="*60)
    
    # Classify IDs
    is_numeric = id_series.apply(lambda x: 1 if str(x).isdigit() else 0).values
    
    results = {}
    
    # Optimize for Numeric IDs
    print("\n[1/2] Optimizing weights for NUMERIC IDs (median score ~526)...")
    numeric_mask = is_numeric == 1
    numeric_preds = [pred[numeric_mask] for pred in predictions_list]
    numeric_y = y_true[numeric_mask]
    
    print(f"  Samples: {numeric_mask.sum()} numeric IDs")
    numeric_weights = optimize_ensemble_weights(numeric_preds, numeric_y, model_names)
    results['numeric'] = numeric_weights
    
    # Optimize for Alphanumeric IDs
    print("\n[2/2] Optimizing weights for ALPHANUMERIC IDs (median score ~22)...")
    alpha_mask = is_numeric == 0
    alpha_preds = [pred[alpha_mask] for pred in predictions_list]
    alpha_y = y_true[alpha_mask]
    
    print(f"  Samples: {alpha_mask.sum()} alphanumeric IDs")
    alpha_weights = optimize_ensemble_weights(alpha_preds, alpha_y, model_names)
    results['alphanumeric'] = alpha_weights
    
    # Compare
    print("\n" + "="*60)
    print("SEGMENTATION SUMMARY")
    print("="*60)
    print(f"\nNumeric IDs weight distribution:")
    for name, w in zip(model_names or [f'Model {i}' for i in range(len(numeric_weights))], numeric_weights):
        print(f"  {name}: {w:.4f}")
    
    print(f"\nAlphanumeric IDs weight distribution:")
    for name, w in zip(model_names or [f'Model {i}' for i in range(len(alpha_weights))], alpha_weights):
        print(f"  {name}: {w:.4f}")
    
    return results


def validate_ensemble_oof():
    """
    Validate ensemble using proper Out-of-Fold (OOF) predictions.
    NO DATA LEAKAGE: each model only predicts on data it was NOT trained on.
    
    DeBERTa Fold 0 â†’ trained on folds {1,2,3,4} â†’ predicts fold 0
    DeBERTa Fold 1 â†’ trained on folds {0,2,3,4} â†’ predicts fold 1
    LightGBM Fold 0 â†’ trained on folds {1,2,3,4} â†’ predicts fold 0
    XGBoost Fold 0 â†’ trained on folds {1,2,3,4} â†’ predicts fold 0
    
    For proper ensemble OOF, we use the overlapping fold (fold 0) where 
    DeBERTa Fold 0, LightGBM, and XGBoost all have held-out predictions.
    Then use fold 1 for DeBERTa Fold 1 separately.
    """
    print("="*60)
    print("PHASE 5: ENSEMBLE VALIDATION (OOF - No Data Leakage)")
    print("="*60)
    
    # Load data
    train = pd.read_csv('Data/train_processed.csv')
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')
    
    # =========================================================
    # Step 1: Generate OOF predictions for each model
    # =========================================================
    print("\n[1/4] Generating Out-of-Fold predictions...")
    
    # --- DeBERTa Fold 0: predicts on fold 0 (its held-out fold) ---
    print("\n  DeBERTa Fold 0 â†’ predicting on fold 0 (OOF)...")
    deberta0, scaler0, feature_cols = load_deberta_model('models/deberta_fold0.pth', device)
    fold0_data = train[train['fold'] == 0].reset_index(drop=True)
    
    if feature_cols and scaler0:
        fold0_features_d0 = scaler0.transform(fold0_data[feature_cols].values)
    else:
        fold0_features_d0 = None
    
    oof_deberta0 = predict_deberta(deberta0, fold0_data['text'].tolist(),
                                    fold0_features_d0, tokenizer, device)
    y_fold0 = fold0_data['score_log'].values
    del deberta0
    torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    # --- DeBERTa Fold 1: predicts on fold 1 (its held-out fold) ---
    print("  DeBERTa Fold 1 â†’ predicting on fold 1 (OOF)...")
    deberta1, scaler1, _ = load_deberta_model('models/deberta_fold1.pth', device)
    fold1_data = train[train['fold'] == 1].reset_index(drop=True)
    
    if feature_cols and scaler1:
        fold1_features_d1 = scaler1.transform(fold1_data[feature_cols].values)
    else:
        fold1_features_d1 = None
    
    oof_deberta1 = predict_deberta(deberta1, fold1_data['text'].tolist(),
                                    fold1_features_d1, tokenizer, device)
    y_fold1 = fold1_data['score_log'].values
    del deberta1
    torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    # --- LightGBM Fold 0: predicts on fold 0 (its held-out fold) ---
    print("  LightGBM â†’ predicting on fold 0 (OOF)...")
    lgb_model = joblib.load('models/lightgbm_fold0.pkl')
    vectorizer = joblib.load('models/tfidf_vectorizer_fold0.pkl')
    X_fold0_tfidf = vectorizer.transform(fold0_data['text'])
    oof_lgb_fold0 = lgb_model.predict(X_fold0_tfidf)
    
    # --- XGBoost v3 Fold 0: predicts on fold 0 (its held-out fold) ---
    print("  XGBoost v3 (Feature Fusion) â†’ predicting on fold 0 (OOF)...")
    xgb_model = joblib.load('models/xgboost_fold0.pkl')
    xgb_scaler = joblib.load('models/xgb_scaler_fold0.pkl')
    xgb_feature_cols = joblib.load('models/xgb_features_fold0.pkl')
    
    # Prepare tabular features for XGBoost
    from scipy.sparse import hstack, csr_matrix
    X_fold0_tab = xgb_scaler.transform(fold0_data[xgb_feature_cols].values)
    X_fold0_tab_sparse = csr_matrix(X_fold0_tab)
    X_fold0_xgb = hstack([X_fold0_tfidf, X_fold0_tab_sparse])
    
    oof_xgb_fold0 = xgb_model.predict(X_fold0_xgb)
    
    # =========================================================
    # Step 2: Evaluate individual model OOF performance
    # =========================================================
    print("\n[2/4] Individual model OOF performance (MAE, original space):")
    
    def calc_mae_original(y_true_log, y_pred_log):
        y_true_orig = np.expm1(y_true_log)
        y_pred_orig = np.expm1(np.clip(y_pred_log, 0, 20))
        return mean_absolute_error(y_true_orig, y_pred_orig)
    
    mae_d0 = calc_mae_original(y_fold0, oof_deberta0)
    mae_d1 = calc_mae_original(y_fold1, oof_deberta1)
    mae_lgb = calc_mae_original(y_fold0, oof_lgb_fold0)
    mae_xgb = calc_mae_original(y_fold0, oof_xgb_fold0)
    
    print(f"  DeBERTa Fold 0 OOF MAE: {mae_d0:.2f}")
    print(f"  DeBERTa Fold 1 OOF MAE: {mae_d1:.2f}")
    print(f"  LightGBM OOF MAE: {mae_lgb:.2f}")
    print(f"  XGBoost OOF MAE: {mae_xgb:.2f}")
    
    # =========================================================
    # Step 3: Optimize ensemble weights on fold 0 
    # (where DeBERTa0, LGB, XGB all have OOF predictions)
    # Then also include DeBERTa1's performance to set its weight
    # =========================================================
    print("\n[3/4] Optimizing ensemble weights on fold 0 OOF predictions...")
    
    # Strategy: Optimize 3-model weights on fold 0 (DeBERTa0, LGB, XGB)
    # Then separately determine DeBERTa1 vs DeBERTa0 blend weight
    
    # First: optimize DeBERTa0 + LGB + XGB on fold 0
    predictions_fold0 = [oof_deberta0, oof_lgb_fold0, oof_xgb_fold0]
    model_names_fold0 = ['DeBERTa Fold 0', 'LightGBM', 'XGBoost']
    weights_fold0 = optimize_ensemble_weights(predictions_fold0, y_fold0, model_names_fold0)
    
    # Now we need to set the final 4-model weights
    # Since DeBERTa F0 and F1 are trained on different folds but same architecture,
    # we blend them equally and distribute the DeBERTa weight
    deberta_total_weight = weights_fold0[0]  # DeBERTa share from fold0 optimization
    
    # Final weights: split DeBERTa weight equally between fold 0 and fold 1
    final_weights = np.array([
        deberta_total_weight / 2,   # DeBERTa Fold 0
        deberta_total_weight / 2,   # DeBERTa Fold 1
        weights_fold0[1],            # LightGBM
        weights_fold0[2]             # XGBoost
    ])
    
    # Normalize to sum to 1
    final_weights = final_weights / final_weights.sum()
    
    print("\n[4/4] Final 4-model ensemble weights:")
    model_names = ['DeBERTa Fold 0', 'DeBERTa Fold 1', 'LightGBM', 'XGBoost']
    for name, weight in zip(model_names, final_weights):
        print(f"  {name}: {weight:.4f}")
    
    # Estimate overall ensemble MAE from fold 0 metrics
    fold0_ensemble = np.average(predictions_fold0, axis=0, weights=weights_fold0)
    fold0_mae = calc_mae_original(y_fold0, fold0_ensemble)
    print(f"\nFold 0 Ensemble MAE (original): {fold0_mae:.2f}")
    
    # Save weights
    os.makedirs('models', exist_ok=True)
    joblib.dump(final_weights, 'models/ensemble_weights.pkl')
    print("\nâœ… Weights saved: models/ensemble_weights.pkl")
    
    return final_weights, fold0_mae


def create_submission(weights=None):
    """
    Create final submission file.
    Phase 7 of POA.
    """
    print("="*60)
    print("PHASE 7: CREATING SUBMISSION")
    print("="*60)
    
    # Load test data
    print("\n[1/5] Loading test data...")
    test = pd.read_csv('Data/test_processed.csv')
    print(f"Test size: {len(test)}")
    
    # Load weights
    if weights is None:
        weights = joblib.load('models/ensemble_weights.pkl')
        print("âœ… Loaded ensemble weights")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Load models
    print("\n[2/5] Loading models...")
    deberta0, scaler0, feature_cols = load_deberta_model('models/deberta_fold0.pth', device)
    deberta1, scaler1, _ = load_deberta_model('models/deberta_fold1.pth', device)
    tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')
    
    lgb_model = joblib.load('models/lightgbm_fold0.pkl')
    xgb_model = joblib.load('models/xgboost_fold0.pkl')
    xgb_scaler = joblib.load('models/xgb_scaler_fold0.pkl')
    xgb_feature_cols = joblib.load('models/xgb_features_fold0.pkl')
    vectorizer = joblib.load('models/tfidf_vectorizer_fold0.pkl')
    print("âœ… All models loaded")
    
    # Generate predictions
    print("\n[3/5] Generating predictions...")
    
    # Prepare features
    if feature_cols:
        test_features0 = scaler0.transform(test[feature_cols].values)
        test_features1 = scaler1.transform(test[feature_cols].values)
    else:
        test_features0 = None
        test_features1 = None
    
    print("  DeBERTa Fold 0...")
    pred_deberta0 = predict_deberta(deberta0, test['text'].tolist(),
                                    test_features0, tokenizer, device)
    del deberta0
    torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    print("  DeBERTa Fold 1...")
    pred_deberta1 = predict_deberta(deberta1, test['text'].tolist(),
                                    test_features1, tokenizer, device)
    del deberta1
    torch.cuda.empty_cache() if torch.cuda.is_available() else None
    
    print("  LightGBM...")
    X_test_tfidf = vectorizer.transform(test['text'])
    pred_lgb = lgb_model.predict(X_test_tfidf)
    
    print("  XGBoost v3 (Feature Fusion)...")
    # Prepare combined features for XGBoost v3
    from scipy.sparse import hstack, csr_matrix
    X_test_tab = xgb_scaler.transform(test[xgb_feature_cols].values)
    X_test_tab_sparse = csr_matrix(X_test_tab)
    X_test_xgb = hstack([X_test_tfidf, X_test_tab_sparse])
    pred_xgb = xgb_model.predict(X_test_xgb)
    
    # Ensemble predictions
    print("\n[4/5] Creating ensemble...")
    predictions_list = [pred_deberta0, pred_deberta1, pred_lgb, pred_xgb]
    
    print("  Weights being applied:")
    model_names = ['DeBERTa Fold 0', 'DeBERTa Fold 1', 'LightGBM', 'XGBoost']
    for name, w in zip(model_names, weights):
        print(f"    {name}: {w:.4f}")
    
    ensemble_pred_log = np.average(predictions_list, axis=0, weights=weights)
    
    # Convert to original space with clipping
    ensemble_pred_log = np.clip(ensemble_pred_log, 0, 20)
    ensemble_pred = np.expm1(ensemble_pred_log)
    
    # Clip negative predictions
    ensemble_pred = np.clip(ensemble_pred, 0, None)
    
    # Round to integers (scores are integers in the dataset)
    ensemble_pred = np.round(ensemble_pred).astype(int)
    
    print(f"\n  Prediction stats:")
    print(f"    Range: [{ensemble_pred.min()}, {ensemble_pred.max()}]")
    print(f"    Median: {np.median(ensemble_pred):.0f}")
    print(f"    Mean: {np.mean(ensemble_pred):.0f}")
    print(f"    P1: {np.percentile(ensemble_pred, 1):.0f}")
    print(f"    P99: {np.percentile(ensemble_pred, 99):.0f}")
    
    # Create submission
    print("\n[5/5] Creating submission file...")
    submission = pd.DataFrame({
        'id': test['id'],
        'score': ensemble_pred
    })
    
    # Validate
    assert submission.shape[0] == 15000, f"Expected 15000 rows, got {submission.shape[0]}"
    assert list(submission.columns) == ['id', 'score'], f"Wrong columns: {submission.columns.tolist()}"
    assert submission['score'].min() >= 0, "Negative scores detected!"
    assert submission.isnull().sum().sum() == 0, "Missing values detected!"
    
    # Save as Solution.csv (competition format)
    os.makedirs('submissions', exist_ok=True)
    submission.to_csv('submissions/Solution.csv', index=False)
    
    print("\nâœ… SUBMISSION CREATED!")
    print(f"  File: submissions/Solution.csv")
    print(f"  Shape: {submission.shape}")
    print(f"  Score range: [{submission['score'].min()}, {submission['score'].max()}]")
    print(f"  Median score: {submission['score'].median():.0f}")
    
    return submission


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Ensemble and submission')
    parser.add_argument('--action', type=str, choices=['validate', 'submit', 'both'],
                       default='both', help='Action to perform')
    
    args = parser.parse_args()
    
    if args.action in ['validate', 'both']:
        weights, avg_mae = validate_ensemble_oof()
        print(f"\nðŸŽ¯ Ensemble validation complete. OOF MAE: {avg_mae:.2f}")
    
    if args.action in ['submit', 'both']:
        submission = create_submission()
        print("\nðŸš€ Ready for submission!")
